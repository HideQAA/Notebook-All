分布式系统
---
一、概念
---
### 1.1 模型 ###

![](https://i.imgur.com/AX7yP1F.png)

**节点：**

在具体的工程项目中，一个节点往往是一个操作系统上的进程。概念上节点是指一个可以独立按照分布式协议完成一组逻辑的程序个体。

**通信：**

节点与节点之间是完全独立、相互隔离的，节点之间传递信息的唯一方式是通过不可靠的网络进行通信。

**存储：**

节点可以通过将数据写入与节点在同一台机器的本地存储设备保存数据。存储、读取数据的节点称为有状态的节点，反之称为无状态的节点。

**异常：**

* 机器宕机：宕机造成的后果通常为该机器上节点不能正常工作。

* 网络异常：

  * 消息丢失：某些节点的直接的网络通信正常或丢包率在合理范围内，而某些节点之间始终无法正常通信，则称这种特殊的网络异常为“网络分化”(network partition)。
  
  * 消息乱序：指由于 IP 网络的存储转发机制、路由不确定性等问题，节点发送的网络消息有一定的概率不是按照发送时的顺序依次到达目的节点，这就要求设计分布式协议时，考虑使用序列号等机制处理网络消息的乱序问题。
  
  * 数据错误：网络上传输的数据有可能发生比特错误，从而造成数据错误。使用一定的校验码机制可以较为简单的检查出网络数据的错误。
  
  * 不可靠的 TCP：TCP 协议保证了 TCP 协议栈之间的可靠的传输，但无法保证两个上层应用之间的可靠通信。TCP协议只能保证同一个 TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。

* 分布式系统的三态：某个节点 A 向另一个节点 B 发送一个消息，节点 B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点 A，那么这个 RPC（远程过程调用） 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称为三态。超时的情况如下：

![](https://i.imgur.com/rRSGexw.png)

出现“超时”时可以通过发起读取数据的操作以验证 RPC 是否成功；也可以在设计分布式协议时将执行步骤设计为可重试的，例如覆盖写就是一种常见的幂等性操作，因为重复的覆盖写最终的结果都相等。

* 存储数据丢失：指节点存储的数据不可被读取或读取出的数据错误，通常只能从其他节点读取、恢复存储的状态。

* 其他无法归类的异常

* 异常处理的黄金原则：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。

### 1.2 副本 ###

**概念：**

指在分布式系统中为数据或服务提供的冗余。

* **数据副本**是分布式系统解决数据丢失异常的唯一手段，当出现某一个节点的存储的数据丢失时，可以从副本上读到数据。
* **服务副本**指数个节点提供某种相同的服务，这种服务一般并不依赖于节点的本地存储，其所需数据一般来自其他节点。

**副本一致性：**

指分布式系统通过副本控制协议，使得从系统外部读取系统内部各个副本的数据在一定的约束条件下相同。

* 强一致性：任何时刻任何用户或节点都可以读到最近一次成功更新的副本数据。

* 单调一致性：任何时刻，任何用户一旦读到某个数据在某次更新后的值，这个用户不会再读到比这个值更旧的值。

* 会话一致性：任何用户在某一次会话内一旦读到某个数据在某次更新后的值，这个用户在这次会话过程中不会再读到比这个值更旧的值。

* 最终一致性：一旦更新成功，各个副本上的数据最终将达到完全一致的状态，但达到完全一致状态所需要的时间不能保障。

* 弱一致性：一旦某个更新成功，用户无法在一个确定时间内读到这次更新的值，且即使在某个副本上读到了新的值，也不能保证在其他副本上可以读到新的值。

### 1.3 衡量指标 ###

* **性能：**常见的性能指标有：系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；系统的响应延迟，指系统完成某一功能需要使用的时间；系统的并发能力，指系统可以同时完成某一功能的能力，通常也用 QPS(query per second)来衡量。

* **可用性：**指系统在面对各种异常时可以正确提供服务的能力。

* **可扩展性：**指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。

* **一致性：**分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。


二、分布式系统原理
---
### 2.1 数据分布方式 ###
---

分布式系统首先要解决的就是如何拆分输入数据，使得分布式系统中的每台机器负责一个子集。

**一、哈希方式：**

按照数据的某一特征计算哈希值，将哈希值与机器中的机器建立映射关系，从而将不同哈希值的数据分布到不同的机器上。数据特征可以是key-value 系统中的 key，也可以是其他与应用业务逻辑相关的值
。

![](https://i.imgur.com/DCjBYl6.png)

按数据属于的用户 id 计算哈希值，集群中的服务器按0到机器数减 1 编号，哈希值除以服务器的个数，结果的余数作为处理该数据的服务器编号。任何时候，任何节点只需要知道哈希函数的计算方式及模的服务器总数，就可以计算出具体数据由哪台服务器来处理。

缺点：

* **可扩展性不高：**一旦集群规模需要扩展，则几乎所有的数据需要被迁移并重新分布。工程中，扩展哈希分布数据的系统时，往往**使得集群规模成倍扩展**，按照数据重新计算哈希，这样原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展。
* **数据倾斜：**某数据特征值的数据严重不均，更为严重的是无论如何扩展集群规模，该用户的数据始终只能由某一台服务器处理，都无法解决这个问题。

![](https://i.imgur.com/LeY02Dh.png)

如上图，当使用用户 id 分数据，且用户 1 的数据特别多时，该用户的数据全部堆积到节点 2 上。
一种思路是重新选择需要哈希的数据特征，如**选择用户 id 与另一个数据维度的组合**作为哈希函数的输入，如这样做，则需要完全重新分布数据，在工程实践中可操作性不高。如果系统处理的每条数据之间没有任何逻辑上的联系，例如一个给定关键词的查询系统，每个关键词之间并没有逻辑上的联系，则可以使用全部数据做哈希的方式解决数据倾斜问题。


**二、按数据范围分布：**

将数据按特征值的**值域范围**划分为不同的区间，使得集群中每台（组）服务器处理不同区间的数据。

![](https://i.imgur.com/OXfg9s5.png)

工程中，为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得**每个区间中服务的数据量尽量的一样多**。

需要使用**专门的服务器在内存中维护数据分布信息**，称这种数据的分布信息为一种元信息。实际工程中，一般也不按照某一维度划分数据范围，而是使用全部数据划分范围，从而避免数据倾斜的问题。

**优点：**可以灵活的根据数据量的具体情况拆分原有数据区间，拆分后的数据区间可以迁移到其他机器，与哈希方式相比非常灵活。当集群需要扩容时，可以随意添加机器，而不限为倍增的方式，只需将原机器上的部分数据分区迁移到新加入的机器上就可以完成集群扩容。

**缺点：**需要维护较为复杂的元信息。

**三、按数据量分布：**

数据量分布数据与具体的数据特征无关，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块，不同的数据块分布到不同的服务器上。

按数据量分布数据也需要记录数据块的具体分布情况，并将该分布信息作为元数据使用元数据服务器管理。

**缺点：**需要管理较为复杂的元信息。

**四、一致性哈希：**

**基本方式：**使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，即哈希函数输出的最大值是最小值的前序。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据。

![](https://i.imgur.com/zV0JOCc.png)

**优点：**扩容时可以任意动态添加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。如上图中增加一个新节点 D，为 D 分配的哈希位置为 3，则首先将节点A 中[3, 4)的数据从节点 A 中拷贝到节点 D，然后加入节点 D 即可。其元信息的量要小的多。

**缺点：**随机分布节点的方式使得很难均匀的分布哈希值域，当一个节点异常时，该节点的压力全部转移到相邻的一个节点，当加入一个新节点时只能为一个相邻节点分摊压力。

**改进：**系统初始时就创建许多虚节点，虚节点的个数一般远大于未来集群中机器的个数，将虚节点均匀分布到一致性哈希值域环上，为每个节点分配若干虚节点。操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点。

一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负载失效节点的压力。一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以负载多个原有节点的压力。

**五、副本与数据分布：**

数据副本的分布方式主要影响系统的可扩展性。

**以机器为单位的副本：**

![](https://i.imgur.com/aOk3pVV.png)

缺点：恢复副本数据时效率不高、不利于提高系统扩展性、不利于系统容错。

**以数据段为单位的副本：**

![](https://i.imgur.com/dUGsgE4.png)

数据段有很多不同的称谓，segment，fragment，chunk，partition 等等。将数据分成数据段，以数据段为单位来管理副本。

优点：

* 一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本机器中，从而可以从整个集群同时拷贝恢复数据。
* 如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。
* 设集群规模为 N 台机器，当加入一台新的机器时，只需从各台机器上迁移 1/N – 1/N+1 比例的数据段到新机器即实现了新的负载均衡。

在工程中，完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也相应增大。折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样做可以将副本粒度控制在一个较为合适的范围内。

**六、本地化计算：**

分布式系统中，除了要解决大规模存储问题，更需要解决大规模的计算问题。而且数据的分布方式也深深影响着计算的分布方式。

分布式系统中的计算节点和存储节点可以在同一物理机器上，也可以在不同的物理机器上。若在不同的物理机器上，则计算的数据需要通过网络传输，网络带宽会成为系统的总体瓶颈。

本地化计算相应诞生：将计算尽量调度到与存储节点在同一台物理机器上的计算节点上进行。这体现了一种重要的分布式调度思想：移动数据不如移动计算。

**七、数据分布方式的选择：**

数据分布方式是如果可以灵活组合使用，往往可以兼备各种方式的优点，收到较好的综合效果。

几个常见的分布式系统的数据分布方式：

![](https://i.imgur.com/ojUG1o7.png)

### 2.2 基本副本协议 ###
---

副本控制协议指按特定的协议流程控制**副本数据的读写行为**，使得副本满足一定的可用性和一致性要求的分布式协议。可用性意味着副本控制协议要具有一定的对抗异常状态的容错能力。分为**中心化副本控制协议**和**去中心化副本控制协议**。

* 中心化副本控制协议：由一个中心节点协调副本数据的更新、维护副本之间的一致性。优点是协议相对较为简单，缺点是存在一定的停服务时间。 **primary-secondary**（也称 primary-backup）是一种非常常用的中心化副本控制协议。

![](https://i.imgur.com/TFvxbzZ.png)

* 去中心化副本控制协议：没有中心节点，协议中所有的节点都是完全对等的，节点之间通过平等协商达到一致。缺点是协议过程通常比较复杂，效率和性能比较低。**Paxos** 是唯一在工程中得到应用的强一致性去中心化副本控制协议。

![](https://i.imgur.com/cmcSDH7.png)

**一、primary-secondary 协议：**

副本被分为两大类，其中有且仅有一个副本作为 primary 副本，除 primary 以外的副本都作为 secondary 副本。维护 primary 副本的节点作为中心节点，中心节点负责维护数据的更新、并发控制、协调副本的一致性。

Primary-secondary 类型的协议一般要解决四大类问题：**数据更新流程**、**数据读取方式**、**Primary副本的确定和切换**、**数据同步（reconcile）**。

**1. 数据更新基本流程：**

![](https://i.imgur.com/TWOtQra.png)

* 数据更新都由 primary 节点协调完成；

* 外部节点将更新操作发给 primary 节点；

* primary 节点进行并发控制即确定并发更新操作的先后顺序；

* primary 节点将更新操作发送给 secondary 节点（往往发送的也是更新的数据，受限于 primary 总的出口网络带宽，有些系统使用接力的方式同步数据，可能会有异常，常见的是基于 Quorum 的副本控制机制）；

* primary 根据 secondary 节点的完成情况决定更新是否成功并将结果返回外部节点。

**2. 数据读取方式：**

* 由于数据的更新流程都是由 primary 控制的，primary 副本上的数据一定是最新的，所以如果始终**只读 primary 副本的数据**，可以实现强一致性。如果按照数据段为单位维护副本，仅有 primary副本提供读服务在很多场景下并不会造成机器资源浪费。那么每台机器上都有一些数据的 primary 副本，也有另一些数据段的 secondary 副本。从而大部分服务器实际都提供读写服务。

* 由 primary 控制节点 secondary 节点的**可用性**。当 primary 更新某个 secondary 副本不成功时，primary 将该 secondary 副本标记为不可用，从而用户不再读取该不可用的副本。不可用的secondary 副本可以继续尝试与 primary 同步数据，当与 primary 完成数据同步后，又可以再标记为可用。

**3. primary 副本的确定与切换：**

在原primary 副本所在机器出现宕机等异常时，需要有某种机制切换 primary 副本，使得某个 secondary副本成为新的 primary 副本。

* 首先确定节点的状态以发现原 primary 节点异常（一种基于 Lease 机制确定节点状态的方法）。

* 切换 primary 后，不能影响副本的一致性，一种直观的方式是切换的新primary的副本数据必须与原primary的副本一致。

* 如何确定一个 secondary 副本使得该副本上的数据与原primary 一致（一种基于 Quorum 机制确定新 primary 的方法）。

一旦 primary 异常，最多需要 10 秒级别的发现时间，系统才能开始 primary 的切换。意味着这段时间不能提供更新服务，可以看出这种协议最大的缺点就是由于 primary 切换带来的一定的停服务时间。

**4. 数据同步：**

当遇到 secondary 副本与 primary 不一致的时候，此时，不一致的 secondary 副本需要与 primary 进行同步。

不一致的形式（解决办法）：

* 网络分化等异常，导致 secondary 上的数据落后于 primary 上的数据（回放 primary 上的操作日志，如redo 日志，从而追上 primary 的更新进度）；

* 在某些协议下，secondary 上的数据有可能是脏数据，需要被丢弃（设计的分布式协议尽量不产生脏数据，也可以设计一些基于 undo 日志的方式从而可以删除脏数据）；

* secondary 是一个新增加的副本，完全没有数据，需要从其他副本上拷贝数据（对某一刻的 primary 副本数据形成快照，然后拷贝快照，再使用回放日志的方式追快照形成后的更新操作）；

**二、工程中副本协议**

工程中大量的副本控制协议都是 primary-secondary 型协议。

















### 2.3 Lease 机制 ###
---
Lease 机制是最重要的分布式协议，广泛应用于各种实际的分布式系统中。

**一、基于 lease 的分布式 cache 系统**

分布式系统中有一个中心服务器节点，系统中其他的节点通过访问中心服务器节点读取、修改其上的元数据，中心服务器节点的性能成为系统的瓶颈。针对这种情况，设计一种元数据 cache，在各个节点上 cache 元数据信息，从而减少对中心服务器节点的访问，提高性能。对 cache 有以下的要求：

* 各个节点上 cache 的数据始终与中心服务器上的数据一致；
*  cache 系统要能最大可能的处理节点宕机、网络中断等异常。

lease 机制的基本原理：

中心服务器在向各节点发送数据时同时向节点颁发一个 lease，每个 lease 具有一个有效期，在 lease 的有效期内，中心服务器保证不会修改对应数据的值。因此，节点收到数据和 lease 后，将数据加入本地 Cache，一旦对应的 lease 超时，节点将对应的本地 cache 数据删除。在中心服务器的角度来看，当其修改数据的时候，首先阻塞所有新的读请求，并等待之前为该数据发出的所有lease 超时过期，然后修改数据的值。

lease 机制可以容错的关键：

服务器一旦发出数据及 lease，无论客户端是否收到，也无论后续客户端是否宕机，也无论后续网络是否正常，服务器只要等待 lease 超时，就可以保证对应的客户端节点不会再继续 cache 数据，从而可以放心的修改数据而不会破坏 cache 的一致性。

优化：

* 服务器在修改元数据时首先要阻塞所有新的读请求。优化：服务器在进入修改数据流程后，一旦收到读请求则只返回数据但不颁发 lease。

* 服务器在修改元数据时需要等待所有的 lease 过期超时，从而造成修改元数据的操作时延大大增大。优化：在等待所有的 lease 过期的过程中，服务器主动通知各个持有lease的节点放弃lease并清除cache中的数据，如果服务器收到客户端返回的确认放弃lease的消息，则服务器不需要在等待该 lease 超时。


**二、lease 机制的分析**

抽象定义：在 lease 有效期内，颁发者一定信守承诺；在 lease 有效期外，接收方一定不能继续使用承诺。

Lease 机制依赖于有效期，这就要求颁发者和接收者的时钟是同步的。对于时钟不同步，实践中的通常做法是将颁发者的有效期设置得比接收者的略大，只需大过时钟误差就可以避免对 lease 的有效性的影响。

**三、基于 lease 机制确定节点状态**

背景：节点 Q 负责判断节点 A、B、C的状态，一旦 Q 发现 primary 异常，节点 Q 将选择另一个节点作为 primary，最开始时节点 A为 primary，B、C 为 secondary。节点 A、B、C 周期性的向 Q 发送心跳信息，假如节点 Q 收不到节点 A 的心跳，除了 A 发生异常外，还可能网络出现问题，甚至 Q 本身出现状况。此时 Q 会认为节点 A 异常，重新选择节点 B 作为新的 primary，出现**双主问题**。

改进：利用 lease 机制，由中心节点向其他节点发送 lease，若某个节点持有有效的 lease，则认为该节点正常可以提供服务。节点 Q 可以给 primary 节点一个特殊的 lease，表示节点可以作为 primary 工作，一旦节点 Q 希望切换新的 primary，则只需等前一个 primary 的 lease 过期，则就可以安全的颁发新的 lease 给新的primary 节点。

**四、lease 有效期时间选择**

使用 lease 确定节点状态时，若 lease 时间过短，有可能造成网络瞬断时节点收不到 lease 从而引起服务不稳定，若 lease 时间过长，则一旦某节点宕机异常，需要较大的时间等待 lease 过期才能发现节点异常。**工程中，常选择的 lease 时长是 10 秒级别**。







### 2.4 Quorum 机制 ###
---

Quorum 机制是一种简单有效的副本管理机制。每个更新操作记为 Wi ，i为更新操作单调递增的序号。Vi 记为每次更新操作执行后的不同数据版本。

**一、Write-all-read-one**

为一种最简单的副本控制规则，更新时写所有的副本，只有在所有的副本上更新成功，才认为更新成功，从而保证所有的副本一致，这样在读取数据时可以读任一副本上的数据。

WARO 读服务的可用性较高，但更新服务的可用性不高：由于更新操作需要在所有的 N 个副本上都成功，更新操作才能成功，所以一旦有一个副本异常，更新操作失败，更新服务不可用。另一方面，N 个副本中只要有一个副本正常，系统就可以提供读服务。对于更新服务，虽然有 N 个副本，但系统无法容忍任何一个副本异常。对于读服务而言，当有 N 个副本时，系统可以容忍 N-1 个副本异常。


**二、Quorum 定义**

将 WARO 条件进行松弛，Quorum 的机制如下：

![](https://i.imgur.com/ChnhIAR.png)

当某次更新操作 Wi 一旦在所有 N 个副本中的 W 个副本上都成功，则就称该更新操作为“成功提交的更新操作”，对应的数据为“成功提交的数据”。令 R>N-W，由于更新操作 Wi 仅在 W 个副本上成功，所以在读取数据时，最多需要读取 R 个副本则一定能读到 Wi  更新后的数据 Vi 。

仅有 quorum 机制时无法确定最新已成功提交的版本号，除非将最新已提交的版本号作为元数据由特定的元数据服务器或元数据集群管理。

**Quorum 机制的三个系统参数 N、W、R 控制了系统的可用性。**

**三、读取最新成功提交的数据**

Quorum 机制中，读取 R 个副本时，一定可以读到最新的成功提交的数据，但由于有不成功的更新情况存在，仅仅读取 R 个副本却不一定能确定哪个版本的数据是最新的已提交的数据。

![](https://i.imgur.com/PJy9orE.png)

如上图，在 N=5，W=3，R=3 的系统中，（a）最新的成功提交的副本应该是 V2 ，因为从全局看 V2 已经成功更新了 3 个副本，但是（b）中的 V2 是一个未成功提交的版本，在上图中当读到 （ V2 V1 V1 ）时，可以肯定的是最新的成功提交的数据要么是 V1 要么是 V2 ，却没办法确定究竟是哪一个？

解决办法，对读取条件进一步加强，步骤如下：

* 限制提交的更新操作必须严格递增，即只有在前一个更新操作成功提交后才可以提交后一个更新操作，从而成功提交的数据版本号必须是**连续增加的**

* 读取 R 个副本，对于 R 个副本中版本号最高的数据：
  
   * 若已存在 W 个，则该数据为最新的成功提交的数据；
   * 若存在个数据少于 W 个，则继续读取其他副本，直若成功读取到 W 个该版本的副本，则该数据为最新的成功提交的数据；若不满足则 R 中版本号第二大的为最新的成功提交的副本。


在单纯使用 Quorum 机制时，若要确定最新的成功提交的版本，最多需要读取 N 个副本，实际工程中，应该尽量通过其他技术手段，回避通过 Quorum 机制读取最新的成功提交的版本，如当 quorum 机制与 primary-secondary 控制协议结合使用时，可以通过读取 primary 的方式读取到最新的已提交的数据。


**四、基于 Quorum 机制选择 primary**

primary 负责进行更新操作的同步工作，primary 成功更新 W 个副本(含 primary 本身)后向用户返回成功。

当 primary 异常时，需要选择出一个新的 primary，之后 secondary 副本与 primary 同步数据。引入 quorum 机制后，中心节点读取 R 个副本，选择 R 个副本中版本号最高的副本作为新的 primary。新 primary 与至少 W 个副本完成数据同步后作为新的 primary 提供读写服务。

在 N=5，W=3，R=3 的系统中， V1 是系统的最新的成功提交的数据 ， V2 是一个处于中间状态的未成功提交的数据。

![](https://i.imgur.com/qImeI3m.png)

情况1，读取到的版本号为（V1 V1 V1 ），则任选一个副本作为primary，把之前的 V2 作为脏数据处理。

![](https://i.imgur.com/WQGBuRw.png)

情况2，读取到的版本号为（V2 V1 V1 ），则选取版本号为 v2 的副本作为新的 primary ，一旦新 primary 与其他 2 个副本完成数据同步，则符合 V2  的副本个数达到 W 个，成为最新的成功提交的副本。





### 2.5 日志技术 ###
---

日志技术是宕机恢复的主要技术之一，

**一、Redo Log**

**更新流程：**

* 将更新操作的结果（例如 Set K1=1，则记录 K1=1）以追加写（append）的方式写入磁盘的日志文件

* 按更新操作修改内存中的数据

* 返回更新成功

**宕机恢复：**

* 从头读取日志文件中的每次更新操作的结果，用这些结果修改内存中的数据

**缺点：**需要回放所有 redo 日志，效率较低，解决这一问题的方法即引入 check point 技术，check point 技术的过程即将内存中的数据以某种易于重新加载的数据组织方式完整的 dump 到磁盘，从而减少宕机恢复时需要回放的日志数据。

**check point：**

* 向日志文件中记录“Begin Check Point”

* 将内存中的数据以某种易于重新加载的数据组织方式 dump 到磁盘上

* 向日志文件中记录“End Check Point”

**基于 check point 的宕机恢复流程：**

* 将 dump 到磁盘的数据加载到内存

* 从后向前扫描日志文件，寻找最后一个“End Check Point”日志

* 从最后一个“End Check Point”日志向前找到最近的一个“Begin Check Point”日志，并回
放该日志之后的所有更新操作日志 

**二、 No Redo/No undo Log**

![](https://i.imgur.com/aZQgMxk.png)

0/1 目录技术中有两个目录结构，称为目录 0 和目录 1 ，另有一个结构称为主记录记录当前正在使用的目录（活动目录），要么记录使用目录 0，要么记录使用目录 1 。目录 0 或目录 1 中记录了各个数据的在日志文件中的位置。0/1 目录的数据**更新过程始终在非活动目录**上进行。

**0/1 目录数据更新流程：**

* 将活动目录完整拷贝到非活动目录。

* 对于每个更新操作，新建一个日志项纪录操作后的值，并在非活动目录中将相应数据的位置修改为新建的日志项的位置。

* 原子性修改主记录：反转主记录中的值，使得非活动目录生效。 


在工程中，0/1 目录的思想运用非常广泛，其形式也不局限在上述流程中，可以是内存中的两个数据结构来回切换，也可以是磁盘上的两个文件目录来回生效切换。


### 2.6 两阶段提交协议 ###
---

两阶段提交协议是一种经典的强一致性中心化副本控制协议，该协议中有几个很经典的问题。在该协议中，参与的节点分为两类：一个中心化协调者节点（coordinator）和 N 个参与者节点（participant）。

**一、流程描述**

在第一阶段，协调者询问所有的参与者是否可以提交事务（请参与者投票），所有参与者向协调者投票。在第二阶段，协调者根据所有参与者的投票结果做出是否事务可以全局提交的决定，并通知所有的参与者执行该决定。在一个两阶段提交流程中，参与者不能改变自己的投票结果。两阶段提交协议的可以全局提交的前提是所有的参与者都同意提交事务。

**两阶段提交协调者流程：**

* 写本地日志“begin_commit”， 并进入 WAIT 状态；

* 向所有参与者发送“prepare 消息”；
* 等待并接收参与者发送的对“prepare 消息”的响应；
 
 * 若收到任何一个参与者发送的“vote-abort 消息”；
   
     * 写本地“global-abort”日志，进入 ABORT；
     * 向所有的参与者发送“global-abort 消息”；
     * 进入 ABORT 状态；

 * 若收到所有参与者发送的“vote-commit”消息；
     * 写本地“global-commit”日志，进入 COMMIT 状态；
     * 向所有的参与者发送“global-commit 消息”；

* 等待并接收参与者发送的对“global-abort 消息”或“global-commit 消息”的确认响应消息，
一旦收到所有参与者的确认消息，写本地“end_transaction” 日志流程结束。

**两阶段提交参与者流程：**

* 写本地日志“init”记录，进入 INIT 状态；

* 等待并接受协调者发送的“prepare 消息”，收到后

 * 若参与者可以提交本次事务
 
      * 写本地日志“ready”，进入 READY 状态；
      * 向协调者发送“vote-commit”消息；
      * 等待协调者的消息；
      * 写本地日志“abort/commit”，进入 ABORT/COMMIT  状态;
      * 向协调者发送对“global-abort/global-commit”的确认消息;

 * 若参与者无法提交本次事务
      * 写本地日志“abort”，进入 ABORT 状态
      * 向协调者发送“vote-abort”消息
      * 流程对该参与者结束
      * 若后续收到协调者的“global-abort”消息可以响应

* 即使流程结束，但任何时候收到协调者发送的“global-abort”消息或“global-commit”消息
也都要发送一个对应的确认消息。

**二、异常处理**

**宕机恢复：**

宕机恢复后，首先通过日志查找到宕机前的状态。

* **协调者宕机恢复：**
 
日志中最后是“begin_commit”记录，协调者处于 WAIT 状态，不清楚有没有发送过“prepare 消息”，但协调者一定还没有发送过“global-commit 消息”或“global-abort 消息”。 **协调者可以重新发送“prepare 消息”继续两阶段提交流程**

日志中最后是“global-commit”或“global-abort”记录，说明宕机前协调者处于 COMMIT 或 ABORT 状态。此时协调者只需**重新向所有的参与者发送“global-commit 消息”或“global-abort 消息”**就可以继续两阶段提交流程。

* **参与者宕机恢复：**

如果日志中最后是“init”记录，说明参与者处于 INIT 状态，还没有对本次事务做出投票选择，**参与者可以继续流程等待协调者发送的“prepare 消息”。**

如果日志中最后是“ready”记录，说明参与者处于 REDAY 状态，此时说明参与者已经就本次事务做出了投票选择，但宕机前参与者是否已经向协调者发送“vote-commit”消息并不可知。所以此时参与者**可以向协调者重发“vote-commit”**，并继续协议流程。

如果日志中最后是“commit”或“abort”记录，说明参与者已经收到过协调者的“global-commit消息”（处于 COMMIT 状态）或者“global-abort 消息”（处于 ABORT 状态）。至于是否向协调者发送过对“global-commit”或“global-abort”的确认消息则未知。但即使没有发送过确认消息，由于协调者会不断重发“global-commit”或“global-abort”，**只需在收到这些消息时发送确认消息既可**，不影响协议的全局一致性。

**响应超时：**

* 协调者在 WAIT 状态超时：协调者可以选择直接放弃整个事务，向所有参与者发送“global-abort”消息，进入 ABORT 状态。

* 协调者在 COMMIT 或 ABORT 状态超时：协调者只能不断重发“global-commit” 或“global-abort”消息给尚未响应的参与者，直到所有的参与者都发送响应。

* 参与者在 INIT 状态超时：参与者可以进入 ABORT 状态

* 参与者在 READY 状态超时：参与者只能不断重发“vote-commit”消息，直到收到协调者的“global-commit”或“global-abort”消息后流程才可继续。

**三、协议分析**

**两阶段提交协议在工程实践中真正使用的较少**，有以下的缺点：

* 两阶段提交协议的容错能力较差，在某些情况下存在流程无法执行下去的情况，且也无法判断流程状态。

* 两阶段提交协议的性能较差，一次成功的两阶段提交协议流程中，协调者与每个参与者之间至少需要两轮交互 4 个消息“prepare”、 “vote-commit”、 “global-commit”、 “确认 global-commit”，过多的交互次数会降低性能。



### 2.7 基于 MVCC 的分布式事务 ###
---

MVCC 即多个不同版本的数据实现并发控制的技术，实现分布式事务，基本思想是为每次事务生成一个新版本的数据，在读数据时选择不同版本的数据即可以实现对事务结果的完整性读取，每个事务都是基于一个已生效的基础版本进行更新。

![](https://i.imgur.com/PUrhK60.png)












































### 2.8 Paxos 协议 ###
---

### 2.9 CAP 理论 ###
---


